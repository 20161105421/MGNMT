data_configs:
  lang_pair:
    - "de-en"
    - "en-de"
  train_data:
    - "unittests/data/train.de"
    - "unittests/data/train.en"
  train_data_synthetic:
    forward:
      - "unittests/data/train.de"
    backward:
      - "unittests/data/train.en"
  valid_data:
    - "unittests/data/dev.de"
    - "unittests/data/dev.en"
  bleu_valid_reference:
    - "unittests/data/dev.en"
    - "unittests/data/dev.de"
  vocabularies:
    - type: "bpe"
      dict_path: "unittests/data/vocab.bpe.32000.json"
      max_n_words: 500
      codes: "unittests/data/bpe.32000"
    - type: "bpe"
      dict_path: "unittests/data/vocab.bpe.32000.json"
      max_n_words: 500
      codes: "unittests/data/bpe.32000"
  max_len:
    - 20
    - 20
  num_refs:
    - 1
    - 1
  eval_at_char_level:
    - false
    - false

model_configs:
  model: MGNMT
  n_layers: 2
  n_head: 3
  dim_per_head: 7
  d_word_vec: 24
  d_model: 24
  d_inner_hid: 17
  latent_size: 10
  dropout: 0.1
  latent_type: input
  bridge_type: zero
  tie_input_output_embedding: true
  tie_source_target_embedding: true
  label_smoothing: 0.1
  # kl-annealing
  anneal_function: logistic
  k: 0.0025
  x0: 4000
  max_kl_weight: 1.0
  step_kl_weight: ~
  kl_factor: 1.0
  # unk dropout
  unk_rate: 0.3
  unk_schedule: fixed
  src_wd: true
  tgt_wd: true
  # decoding
  decoding_iterations: 3
  # # loss weights
  # duality_weight: 0.0
  # tm_lm_tying_weight: 0.0
  # inferrer
  inferrer_stop_grad_input: true
  inferrer_type: RNN

optimizer_configs:
  optimizer: "adam"
  learning_rate: 2.0
  grad_clip: 0.0
  optimizer_params: ~ # other arguments for optimizer.
  schedule_method: noam
  scheduler_configs:
    d_model: 24
    warmup_steps: 4000

training_configs:
  seed: 1234
  max_epochs: 2
  shuffle: false
  use_bucket: true # Whether to use bucket. If true, model will run faster while a little bit performance regression.
  buffer_size: 100 # Only valid when use_bucket is true.
  batch_size: 20
  batching_key: "tokens"
  update_cycle: 2
  valid_batch_size: 3
  disp_freq: 100
  save_freq: 1
  num_kept_checkpoints: 100
  num_kept_best_checkpoints: 3
  loss_valid_freq: &decay_freq 2
  bleu_valid_freq: 10000
  bleu_valid_batch_size: 3
  bleu_valid_warmup: 1
  bleu_valid_configs:
    - max_steps: 150
      beam_size: 5
      alpha: 0.0
      sacrebleu_args: "--tokenize none -lc"
      postprocess: false
    - max_steps: 150
      beam_size: 5
      alpha: 0.0
      sacrebleu_args: "--tokenize none -lc"
      postprocess: false
  early_stop_patience: 20
  # train_synthetic_update_mode: joint  # joint or separate
  # train_synthetic_directions:
  #   - forward
  #   - backward
  # train_synthetic_warmup_step: 15000
  # train_synthetic_mode: hybrid # finetune or hybrid
  # training_iterations:
  #   joint: 1
  #   synthetic: 1
  # train_synthetic_options:
  #   monolingual_cycle: true
